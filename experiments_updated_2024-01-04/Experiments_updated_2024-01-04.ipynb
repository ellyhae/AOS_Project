{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "368a6f96-d68b-4151-b9d1-cd5a92c88cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# added imports\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import functional as TF\n",
    "import matplotlib.pyplot as plt\n",
    "from pytorch_msssim import ssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c97e72fe-69a9-450a-8bb2-cc02f4a639f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ced7f47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d03d459a-5db6-4c9b-9bb6-0abf0a0a5e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import FusionDenoiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff490bbd-3c2d-4c9a-9184-e32fc27b26cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(43)\n",
    "np.random.seed(43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bc23007-064e-4eae-b10c-9056dbf02849",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import FocalDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5e8986e-4d59-488c-8dc7-367392406e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = FocalDataset(input_channels=1, output_channels=3, augment=True, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7607fccd-4548-4d21-b981-3711dacab335",
   "metadata": {},
   "outputs": [],
   "source": [
    "stack, gt = ds[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7457b13-6fed-4d80-b363-460c11e78877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1, 512, 512])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e6e968a-c0eb-459a-8220-bdd80cd66d7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 512, 512])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb83bb45-c094-4d20-9eff-fffa21dcbc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(1, 3, figsize=(18,10), sharey=True)\n",
    "\n",
    "# axes[0].imshow(np.moveaxis(stack[0].numpy(), 0,-1), cmap='gray')\n",
    "# axes[1].imshow(np.moveaxis(stack[1].numpy(), 0,-1), cmap='gray')\n",
    "# axes[2].imshow(np.moveaxis(gt.numpy(), 0,-1), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffd6b3c1-8dcd-4532-b645-3fcefc678972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization constants used by pretrained IFCNN\n",
    "# only really needed when using pretrained weights\n",
    "# may be a good idea to adapt IFCNN to be similar to SwinIR in the way it works with image pixel range\n",
    "mean = [0.485, 0.456, 0.406] #[0.46]*3\n",
    "std = [0.229, 0.224, 0.225] #[0.225]*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "672f5f44-992a-4582-85de-a054f438dac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_mode = 2  # random, lytro, integrals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe16cef4-c0a3-4490-a33d-a28f684cc01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://discuss.pytorch.org/t/how-to-add-noise-to-mnist-dataset-when-using-pytorch/59745/2\n",
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0., std=1.):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "        \n",
    "    def __call__(self, tensor):\n",
    "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d753f09f-0755-4b76-b485-94402eac4b75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 1, 128, 128])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if trial_mode == 0:\n",
    "    inp = torch.randn(16, 4, 1, 128, 128)\n",
    "    inp = inp.repeat(1, 1, 3, 1, 1) # repeat gray channels as faux rgb\n",
    "elif trial_mode == 1:\n",
    "    trans = transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=3),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((128, 128), antialias=True),\n",
    "    ] + ([AddGaussianNoise(std=0.015)] if trial_mode == 1 else []) + [   # Add noise to lytro images to see SwinIR effect\n",
    "        transforms.Normalize(mean=mean, std=std),\n",
    "    ])\n",
    "    \n",
    "    if trial_mode == 1:\n",
    "        paths = glob('./lytro/*.jpg')\n",
    "        \n",
    "    inp = torch.stack([trans(Image.open(p)) for p in paths])[None, :]\n",
    "elif trial_mode == 2:\n",
    "    inp = transforms.Resize((128, 128), antialias=True)(ds[5][0])[None, :]\n",
    "    \n",
    "# inp = inp.cuda()\n",
    "inp = inp.to(device)\n",
    "inp.shape  # batch, n_focal_lengths, channels, dim1, dim2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e77224dd-cf2f-403f-bbaf-194e8c2672a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(inp[0,0,0].cpu(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bca7d423-3bd2-427a-b57e-dc93400cc3d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.38773103657223723, 0.12404649765057327)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp.mean().item(), inp.std().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1cf56a",
   "metadata": {},
   "source": [
    "## Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e3cc6d",
   "metadata": {},
   "source": [
    "Below one can see the **updated steps** of\n",
    "- Loading the model using a cpu (should also work for gpu altough that wasn't tested). For that also the model.py file was updated.\n",
    "- Defining and using the evaluation metrics, so psnr and ssim for somewhat arbitrary inputs. I have added some sample calls of the functions and used them in the training loop to give insight into how they can be used.\n",
    "- Model training. Since I don't have a strong machine available, I had to stop the training early. I hope after updating the target values, the training works out fine.\n",
    "\n",
    "**Attention:** \n",
    "- I have used a pseudo-target and not the target we have to use. This is relevant for the CustomDataset() class since that one has to be updated with the correct target.\n",
    "- There is no train/validation-split. I just copied the validation set with the training set for simplicity, since we don't have data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b3817d",
   "metadata": {},
   "source": [
    "### Create model\n",
    "\n",
    "Get missing file from here: https://github.com/uzeful/IFCNN/blob/master/Code/snapshots/IFCNN-MAX.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9c98365-6095-47fd-9de6-6931ebf97543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pretrained weights available for grayscale Swin V2\n"
     ]
    }
   ],
   "source": [
    "# limited to img_size=128 when using pretrained=True, as SwinIR does not have pretrained weights for that size\n",
    "# model = FusionDenoiser(img_size=128, swin_version='V2', use_checkpoint=True, pretrained=True)\n",
    "model = FusionDenoiser(img_size=128, swin_version='V2', use_checkpoint=True, pretrained=True)\n",
    "model = model.to(device).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93a7a01",
   "metadata": {},
   "source": [
    "### Create evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e234f0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 1, 128, 128])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37837190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model uses 3 color channels, so we need to repeat the one grayscale dimension 3 times\n",
    "inp = inp.repeat(1, 1, 3, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "397cc92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = inp.float()\n",
    "output = model(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d3ce6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_output = tuple(o * 0.98 for o in output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "328c1611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR: 39.614797592163086\n"
     ]
    }
   ],
   "source": [
    "def calculate_psnr_tensor(target, prediction):\n",
    "    mse = torch.mean((target - prediction) ** 2)\n",
    "    psnr = 20 * torch.log10(torch.tensor(255.0)) - 10 * torch.log10(mse)\n",
    "    return psnr.item()  # Convert to Python scalar\n",
    "\n",
    "psnr_values = [calculate_psnr_tensor(o, s) for o, s in zip(output, scaled_output)]\n",
    "# ssim_values = [calculate_ssim_tensor(o, s) for o, s in zip(output, scaled_output)]\n",
    "\n",
    "average_psnr = sum(psnr_values) / len(psnr_values)\n",
    "# average_ssim = sum(ssim_values) / len(ssim_values)\n",
    "\n",
    "print(\"Average PSNR:\", average_psnr)\n",
    "# print(\"Average SSIM:\", average_ssim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c0bab3c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average SSIM: 0.999765545129776\n"
     ]
    }
   ],
   "source": [
    "def calculate_ssim_tensor(target, prediction):\n",
    "    # ensure the input tensors are float32, as required by pytorch_msssim\n",
    "    target = target.type(torch.float32)\n",
    "    prediction = prediction.type(torch.float32)\n",
    "\n",
    "    # the ssim function expects tensors in the shape of (batch, channel, height, width)\n",
    "    # ensure your tensors are correctly shaped\n",
    "    ssim_value = ssim(target, prediction, data_range=255, size_average=True)  # size_average to return the average of SSIM\n",
    "    return ssim_value.item()  # make it python scalar\n",
    "\n",
    "# Example usage\n",
    "ssim_values = [calculate_ssim_tensor(o, s) for o, s in zip(output, scaled_output)]\n",
    "average_ssim = sum(ssim_values) / len(ssim_values)\n",
    "\n",
    "print(\"Average SSIM:\", average_ssim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c23fbe",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "09bd94ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0f9de0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb6e4481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell needs to be adjusted for our problem and redone.\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, model):\n",
    "        # Flatten the first two dimensions\n",
    "        self.data = data.view(-1, 1, 128, 128)\n",
    "        self.model = model\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "            Attention: I have made up the target! The CustomDataset needs to be transformed if\n",
    "            we want to use it for the project.\n",
    "        '''\n",
    "        inp = self.data[idx].repeat(1, 3, 1, 1)  # Convert to 3-channel image\n",
    "        inp = inp.unsqueeze(0)  # Add batch dimension\n",
    "        with torch.no_grad():\n",
    "            model_output = self.model(inp)\n",
    "            if isinstance(model_output, (list, tuple)):\n",
    "                target = model_output[0]  # Assuming you want to use the first output as the target\n",
    "            else:\n",
    "                target = model_output\n",
    "            target = target * 0.9\n",
    "        return inp.squeeze(0), target.squeeze(0)  # Remove batch dimension for DataLoader compatibility\n",
    "\n",
    "# Assuming model and inp are already defined\n",
    "dataset = CustomDataset(inp, model)\n",
    "\n",
    "# Creating the same dataset for both training and validation\n",
    "train_loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a5ecf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a0d919ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loop (early stopping) done\n",
      "validation loop (early stopping) done\n",
      "Epoch 1/5, Training Loss: 7.80271848042806, Validation Loss: 6.423454920450847, Training PSNR: 1.058592955271403, Training SSIM: 0.041258071859677635, Validation PSNR: 1.0937918027242024, Validation SSIM: 0.041403998931248985\n",
      "training loop (early stopping) done\n",
      "validation loop (early stopping) done\n",
      "Epoch 2/5, Training Loss: 7.269136428833008, Validation Loss: 5.911933898925781, Training PSNR: 1.0714109738667805, Training SSIM: 0.04125023384888967, Validation PSNR: 1.1088080406188965, Validation SSIM: 0.04140225052833557\n",
      "training loop (early stopping) done\n",
      "validation loop (early stopping) done\n",
      "Epoch 3/5, Training Loss: 6.770402272542317, Validation Loss: 5.586081822713216, Training PSNR: 1.0842727025349934, Training SSIM: 0.041259231666723885, Validation PSNR: 1.1190673510233562, Validation SSIM: 0.04140132168928782\n",
      "training loop (early stopping) done\n",
      "validation loop (early stopping) done\n",
      "Epoch 4/5, Training Loss: 6.391652425130208, Validation Loss: 5.354084650675456, Training PSNR: 1.094689925511678, Training SSIM: 0.041273278494675956, Validation PSNR: 1.126743237177531, Validation SSIM: 0.04139753927787145\n",
      "training loop (early stopping) done\n",
      "validation loop (early stopping) done\n",
      "Epoch 5/5, Training Loss: 6.092946370442708, Validation Loss: 5.120999336242676, Training PSNR: 1.1033506393432617, Training SSIM: 0.04128178209066391, Validation PSNR: 1.1347975730895996, Validation SSIM: 0.04138863583405813\n"
     ]
    }
   ],
   "source": [
    "# lists to store metrics for plotting\n",
    "train_losses, val_losses = [], []\n",
    "train_psnrs, val_psnrs = [], []\n",
    "train_ssims, val_ssims = [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss, train_psnr, train_ssim = 0.0, 0.0, 0.0\n",
    "\n",
    "    for inp, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        model_output = model(inp)\n",
    "        if isinstance(model_output, tuple):\n",
    "            model_output = model_output[0]\n",
    "\n",
    "        loss = criterion(model_output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_psnr += calculate_psnr_tensor(target, model_output)\n",
    "        train_ssim += calculate_ssim_tensor(target, model_output)\n",
    "        \n",
    "        print('training loop (early stopping) done')\n",
    "        break   # this needs to be removed and guarantees us that we only train on the first sample, not all.\n",
    "\n",
    "    # average over the epoch and store metrics (training)\n",
    "    train_loss /= len(train_loader)\n",
    "    train_psnr /= len(train_loader)\n",
    "    train_ssim /= len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "    train_psnrs.append(train_psnr)\n",
    "    train_ssims.append(train_ssim)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss, val_psnr, val_ssim = 0.0, 0.0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for inp, target in val_loader:\n",
    "            model_output = model(inp)\n",
    "            if isinstance(model_output, tuple):\n",
    "                model_output = model_output[0]\n",
    "\n",
    "            loss = criterion(model_output, target)\n",
    "            val_loss += loss.item()\n",
    "            val_psnr += calculate_psnr_tensor(target, model_output)\n",
    "            val_ssim += calculate_ssim_tensor(target, model_output)\n",
    "            \n",
    "            print('validation loop (early stopping) done')\n",
    "            break   # this needs to be removed and guarantees us that we only evaluate on the first sample, not all.\n",
    "        \n",
    "    # average over the epoch and store metrics (validation)\n",
    "    val_loss /= len(val_loader)\n",
    "    val_psnr /= len(val_loader)\n",
    "    val_ssim /= len(val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    val_psnrs.append(val_psnr)\n",
    "    val_ssims.append(val_ssim)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {train_loss}, Validation Loss: {val_loss}, Training PSNR: {train_psnr}, Training SSIM: {train_ssim}, Validation PSNR: {val_psnr}, Validation SSIM: {val_ssim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb3ea6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add plotting here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f4daed-76d1-4b83-8527-fce866310bbf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fused' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m fig, axes \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m18\u001b[39m,\u001b[38;5;241m10\u001b[39m), sharey\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m axes[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mimshow(inp[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu(), cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgray\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m axes[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mimshow(fused[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu(), cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgray\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m axes[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mimshow(denoised[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu(), cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgray\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m axes[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mset_title(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOne of the inputs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'fused' is not defined"
     ]
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18,10), sharey=True)\n",
    "\n",
    "axes[0].imshow(inp[0,0,0].cpu(), cmap='gray')\n",
    "axes[1].imshow(fused[0,0].cpu(), cmap='gray')\n",
    "axes[2].imshow(denoised[0,0].cpu(), cmap='gray')\n",
    "\n",
    "axes[0].set_title('One of the inputs')\n",
    "axes[1].set_title('Fused')\n",
    "axes[2].set_title('Denoised')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbc4c0e-ed3e-4347-b8d8-029a25369c78",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "- generate AOS integral data\n",
    "- Dataset with augmentation (random 0°, 90°, 180°, 270° rotation + random flip, maybe random crop, maybe noise). Data seems to be quite abundant, so just rotation and flips may be sufficient\n",
    "- evaluation function\n",
    "- training loop with logging, validation evaluation and checkpointing\n",
    "- self-ensemble for final predictions (not used for training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d20e23-256a-4366-be6e-c1861e68d8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "j = np.arange(2*3*1*2).reshape(2, 3, 1, 2)\n",
    "j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33a11d5-d08b-4dad-9bb8-b56bb63ed1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rots = np.random.randint(4)\n",
    "j = np.rot90(j, num_rots, (-3, -2))\n",
    "\n",
    "if np.random.rand() > 0.5:\n",
    "    j = np.flip(j, -2)\n",
    "\n",
    "if np.random.rand() > 0.5:\n",
    "    j = np.flip(j, -3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48b86f6-9c39-4420-a41c-b9c47013919c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.repeat(j, 3, -2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b18f29-16f3-472c-afbf-14f3c104058c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
