{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "01259d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image_path = r'C:\\Users\\chris\\Documents\\JKU\\ComputerVision\\test'# \\0_5_0_5_integral.tiff' #'test' # can be either a path to a single file or to a directory containing files. 'real_integrals//focal_stack'  # 'real_integrals' # 'test'\n",
    "folder_datatype = 'tiff'  # 'tiff' # \"png\"   # has to be set to tiff for the testfolder\n",
    "get_info_every = 1000\n",
    "make_plots = False #True\n",
    "generate_outputs = True\n",
    "output_folder = 'final_real_world_predictions'\n",
    "model_path = 'tmp/submission_model.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "747ce514",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from utils import calculate_psnr_tensor, calculate_ssim_tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import cv2\n",
    "import json\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "# import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision.transforms import Resize\n",
    "\n",
    "from swin2sr import Swin2SR as Swin\n",
    "from dataset import PositionDataset\n",
    "\n",
    "def load_model(path: str):\n",
    "    model = Swin(img_size=512,\n",
    "                 in_chans=1,\n",
    "                 window_size=8,\n",
    "                 depths=[2, 2, 2, 2],\n",
    "                 num_heads=[4, 4, 4, 4],\n",
    "                 embed_dim=32,\n",
    "                 mlp_ratio=4,\n",
    "                 img_range=1.,\n",
    "                 ape=True,\n",
    "                 use_checkpoint=True).cuda()\n",
    "    \n",
    "    existing_model_state = Path(path)\n",
    "    if not existing_model_state.exists():\n",
    "        print(f\"Model state not found at {existing_model_state.absolute()}\")\n",
    "        exit(1)\n",
    "    \n",
    "    print(f\"Using the existing model state from {existing_model_state.absolute()}\")\n",
    "    model.load_state_dict(torch.load(existing_model_state))\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    return model\n",
    "\n",
    "def tens2img(tensor: torch.Tensor):\n",
    "    '''Move the axes of a 3D Tensor such that it can be plotted as an image'''\n",
    "    return np.moveaxis(tensor.detach().cpu().numpy(), 0,-1)\n",
    "\n",
    "def load_tiff(path: str, focal_idx: list):\n",
    "    '''Load the tiff image from the given path, select the goven focal heights (using the index).\n",
    "    Return a Tensor of shape (num_focal_heights, H, W) with pixel values in the range [0-155]'''\n",
    "    ok, focal_stack = cv2.imreadmulti(path)\n",
    "    if not ok:\n",
    "        raise IOError(f'Failed to load index: {path}')\n",
    "        \n",
    "    focal_stack = np.stack(focal_stack)   # shape (num_focal_lengths, H, W)\n",
    "    focal_stack = focal_stack[focal_idx]\n",
    "    focal_stack = torch.from_numpy(focal_stack)\n",
    "    return focal_stack\n",
    "\n",
    "def load_image(path: str, focal_idx: list):\n",
    "    _, ext = os.path.splitext(path)\n",
    "\n",
    "    if ext.lower() in ['.tiff', '.tif']:\n",
    "        ok, focal_stack = cv2.imreadmulti(path, flags=cv2.IMREAD_ANYDEPTH)\n",
    "        if not ok:\n",
    "            raise IOError(f'Failed to load TIFF: {path}')\n",
    "        focal_stack = np.stack(focal_stack)\n",
    "    elif ext.lower() in ['.png', '.jpg', '.jpeg']:\n",
    "        img = cv2.imread(path, cv2.IMREAD_ANYDEPTH)\n",
    "        if img is None:\n",
    "            raise IOError(f'Failed to load image: {path}')\n",
    "        if len(img.shape) == 2:\n",
    "            img = np.expand_dims(img, axis=-1)\n",
    "        focal_stack = np.array([img])\n",
    "    else:\n",
    "        raise ValueError(f'Unsupported file format: {ext}')\n",
    "\n",
    "    # Select specified focal indices\n",
    "    focal_stack = focal_stack[focal_idx]\n",
    "\n",
    "    # Convert to PyTorch tensor and scale\n",
    "    focal_stack = torch.from_numpy(focal_stack).float()\n",
    "    \n",
    "    if focal_stack.dim() == 3:  # If there are only 3 dimensions, add a channel dimension\n",
    "        focal_stack = focal_stack.unsqueeze(1)\n",
    "\n",
    "    # Ensure consistent dimension order for PyTorch (B, C, H, W)\n",
    "    focal_stack = focal_stack.permute(0, 3, 1, 2)  # Change dimension order\n",
    "    \n",
    "    if ext.lower() in ['.png', '.jpg', '.jpeg']:\n",
    "        focal_stack = focal_stack[0]\n",
    "    \n",
    "    if focal_stack.shape != [1, 512, 512]:\n",
    "        focal_stack = focal_stack.unsqueeze(1)\n",
    "        focal_stack = F.interpolate(focal_stack, size=(512, 512), mode='bilinear')\n",
    "        focal_stack = focal_stack.squeeze(1)\n",
    "    \n",
    "    return focal_stack\n",
    "\n",
    "def preprocess(stack: torch.Tensor):\n",
    "    '''Resize and normalize the images in the stack'''\n",
    "    return Resize(512)(stack.contiguous().div(256))\n",
    "\n",
    "@torch.no_grad()\n",
    "@torch.autocast(device_type='cuda', dtype=torch.float16)\n",
    "def self_ensemble(model: Swin, stack: torch.Tensor, passes:int = 2):\n",
    "    '''Calculate the model output for the given input stack.\n",
    "    The stack is assumed to be of shape (num_focal_height, H, W) with pixel values in the range [0,1].\n",
    "    The stack is assumed to be ordered by ascending focal height, i.e. stack[0] is the closest to 0m (the floor)\n",
    "    The model is assumed to only work on single channel images, so the results for different focal heights are calculated separately and then aggregated later'''\n",
    "\n",
    "    get_rots = lambda x: [torch.rot90(x, i, (-2, -1)) for i in range(4)]\n",
    "    undo_rots = lambda x: [torch.rot90(j, -i, (-2, -1)) for i, j in enumerate(x)]\n",
    "    ### maybe also include flipped versions\n",
    "\n",
    "    single_pass_denoised = None\n",
    "    no_ensemble_denoised = None\n",
    "    denoised = stack\n",
    "    \n",
    "    # feed the output through the model several times\n",
    "    for _ in range(passes):\n",
    "        fixed_preds = []\n",
    "        for integral in denoised:\n",
    "            integral = integral[None,:]  # see each integral as an individual grayscale image\n",
    "\n",
    "            versions = torch.stack(get_rots(integral)).cuda()\n",
    "\n",
    "            preds = model(versions)\n",
    "\n",
    "            if no_ensemble_denoised is None:  # assume the first integral in the stack as the training height\n",
    "                no_ensemble_denoised = preds[0].clone()  # store it's unmodified prediction for comparison\n",
    "\n",
    "            fixed_preds += undo_rots(preds)\n",
    "\n",
    "        denoised = torch.stack(fixed_preds).median(0).values\n",
    "\n",
    "        if single_pass_denoised is None:\n",
    "            single_pass_denoised = denoised.clone()\n",
    "\n",
    "    # safeguard against making results worse\n",
    "    if single_pass_denoised.std() < denoised.std():\n",
    "        denoised = single_pass_denoised\n",
    "    return no_ensemble_denoised, single_pass_denoised, denoised\n",
    "\n",
    "def postprocess(stack: torch.Tensor):\n",
    "    '''Convert the model output to the range [0, 255]'''\n",
    "    return stack.mul(256).clip(0, 255).int()\n",
    "\n",
    "def calculate_metrics(denoised, ground_truth):\n",
    "    dn = denoised[None,:].float()\n",
    "    gt = ground_truth[None,:].float()\n",
    "    loss = nn.functional.l1_loss(dn, gt).item()\n",
    "    psnr = calculate_psnr_tensor(gt, dn, 255.)\n",
    "    ssim = calculate_ssim_tensor(gt, dn, 255.)\n",
    "    return [loss, psnr, ssim]\n",
    "\n",
    "def main():\n",
    "    \n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    if not os.path.exists(os.path.join(output_folder, input_image_path)):\n",
    "        os.makedirs(os.path.join(output_folder, input_image_path))\n",
    "    \n",
    "    model = load_model(model_path)\n",
    "    denoising_passes = 2\n",
    "\n",
    "    focal_idx = [0]\n",
    "    # input_image_path = 'test'    #0_130_2_-5_integral.tiff'\n",
    "\n",
    "    # added different support of datatypes\n",
    "    # every format supported by cv2.imreadmulti(path) is allowed: e.g. png, jpg\n",
    "    # important: write datatype here without a \".\" in the beginning!\n",
    "\n",
    "    # if a path to a directory was given, load all tiff files from there. Useful for calculating the loss over some dataset\n",
    "    if os.path.isdir(input_image_path):\n",
    "        print('Detected folder as input path, loading all files')\n",
    "        image_files = glob(os.path.join(input_image_path, f\"*.{folder_datatype}\"))\n",
    "        datatype = folder_datatype\n",
    "        \n",
    "    # if the path is a file, treat it as if it's a dataset with only one sample. Makes further code more concise\n",
    "    else:\n",
    "        datatype = os.path.splitext(input_image_path)[1]\n",
    "        image_files = [input_image_path]\n",
    "\n",
    "    ## make predictions\n",
    "    multipass = denoising_passes > 1\n",
    "    metrics, ensemble_metrics, multipass_metrics = [], [], []\n",
    "    \n",
    "    # Initialize gt before the loop\n",
    "    # gt = False\n",
    "    \n",
    "    # Loop over all image files. Load each tiff file as a (num_focal_heights, H, W) Tensor, preprocess it and calculate the prediction\n",
    "    for idx, f in enumerate(tqdm(image_files, desc='Calculating output(s)')):\n",
    "        # stack = load_tiff(f, focal_idx)\n",
    "        \n",
    "        stack = load_tiff(f, focal_idx) if f.split('.')[-1]=='tiff' else load_image(f, focal_idx)\n",
    "        stack = preprocess(stack)\n",
    "\n",
    "        no_ensemble_denoised, single_pass_denoised, denoised = map(postprocess, self_ensemble(model, stack, denoising_passes))\n",
    "\n",
    "        # If a ground truth image is present, calculate key metrics. Useful for getting an overview of the model on some dataset\n",
    "        gt_path = f.removesuffix(f\"integral.{datatype}\") + 'gt.png'\n",
    "        gt = os.path.exists(gt_path)\n",
    "        \n",
    "        if gt:\n",
    "            ground_truth = torch.from_numpy(np.moveaxis(cv2.imread(gt_path)[...,[0]], -1, 0)).int().cuda()\n",
    "            metrics.append(calculate_metrics(no_ensemble_denoised, ground_truth))\n",
    "            ensemble_metrics.append(calculate_metrics(single_pass_denoised, ground_truth))\n",
    "            multipass_metrics.append(calculate_metrics(denoised, ground_truth))\n",
    "        \n",
    "        if idx%get_info_every == 0:\n",
    "\n",
    "            # if ground truths were available, print the results\n",
    "            if gt:\n",
    "                loss, psnr, ssim = np.mean(metrics, 0)\n",
    "                ensemble_loss, ensemble_psnr, ensemble_ssim = np.mean(ensemble_metrics, 0)\n",
    "                multipass_loss, multipass_psnr, multipass_ssim = np.mean(multipass_metrics, 0)\n",
    "                \n",
    "                print('\\n                   L1 Loss / PSNR / SSIM:')\n",
    "                print(f'Simple denoised:   {loss:.3f} / {psnr:.3f} / {ssim:.3f}')\n",
    "                print(f'+ Ensemble:        {ensemble_loss:.3f} / {ensemble_psnr:.3f} / {ensemble_ssim:.3f}')\n",
    "                if multipass:\n",
    "                    print(f'+ Multi-Pass [{denoising_passes}]:  {multipass_loss:.3f} / {multipass_psnr:.3f} / {multipass_ssim:.3f}')\n",
    "        \n",
    "        \n",
    "        # plot the last input-denoised pair\n",
    "        fig, axes = plt.subplots(1, 3 + gt + multipass, figsize=(18,8), sharey=True, sharex=True)\n",
    "        \n",
    "        axes[0].imshow(tens2img(stack[[0]]), cmap='gray', vmin=0, vmax=1)\n",
    "        axes[1].imshow(tens2img(no_ensemble_denoised), cmap='gray', vmin=0, vmax=255)\n",
    "        axes[2].imshow(tens2img(single_pass_denoised), cmap='gray', vmin=0, vmax=255)\n",
    "\n",
    "        axes[0].set_title('Input')\n",
    "        axes[1].set_title('Denoised')\n",
    "        axes[2].set_title('+ Self Ensemble')\n",
    "\n",
    "        if multipass:\n",
    "            axes[3].imshow(tens2img(denoised), cmap='gray', vmin=0, vmax=255)\n",
    "            axes[3].set_title(f'+ Multi-Pass [{denoising_passes}]')\n",
    "\n",
    "        if gt:\n",
    "            axes[-1].imshow(tens2img(ground_truth), cmap='gray', vmin=0, vmax=255)\n",
    "            axes[-1].set_title('Ground Truth')\n",
    "                \n",
    "        if generate_outputs:\n",
    "            a5 = axes[3].imshow(tens2img(denoised), cmap='gray', vmin=0, vmax=255)\n",
    "            \n",
    "            image_data = denoised.squeeze().cpu().numpy()  # Adjust as necessary based on tensor dimensions\n",
    "\n",
    "            # Construct the new file path\n",
    "            path_components = f.split(os.path.sep)\n",
    "            path_components[0] = os.path.join(output_folder, os.path.basename(input_image_path))  # Use 'basename' for safety\n",
    "\n",
    "            # Extract the filename without the extension and add '.png'\n",
    "            filename_without_extension = '.'.join(path_components[-1].split('.')[:-1])\n",
    "            path_components[-1] = filename_without_extension + '.png'  # Change file extension to '.png'\n",
    "            new_path = os.path.join(*path_components)\n",
    "\n",
    "            name, ext = os.path.splitext(os.path.basename(f))\n",
    "            new_path = os.path.join(output_folder, name + '.png')\n",
    "\n",
    "            # print(new_path)\n",
    "            \n",
    "            # Ensure the directory exists before saving the image\n",
    "            save_dir = os.path.dirname(new_path)\n",
    "            if not os.path.exists(save_dir):\n",
    "                os.makedirs(save_dir)\n",
    "\n",
    "            # Save the image data to the new path\n",
    "            im = Image.fromarray(image_data.astype(np.uint8))\n",
    "            im.save(new_path)\n",
    "            #plt.imsave(new_path, image_data, cmap='gray', vmin=0, vmax=255, dpi=500)\n",
    "        \n",
    "        if make_plots and idx%get_info_every == 0:\n",
    "            plt.show()\n",
    "        \n",
    "        plt.close()\n",
    "\n",
    "    # if ground truths were available, print the results\n",
    "    if gt:\n",
    "        loss, psnr, ssim = np.mean(metrics, 0)\n",
    "        ensemble_loss, ensemble_psnr, ensemble_ssim = np.mean(ensemble_metrics, 0)\n",
    "        multipass_loss, multipass_psnr, multipass_ssim = np.mean(multipass_metrics, 0)\n",
    "        \n",
    "        print('\\n                   L1 Loss / PSNR / SSIM:')\n",
    "        print(f'Simple denoised:   {loss:.3f} / {psnr:.3f} / {ssim:.3f}')\n",
    "        print(f'+ Ensemble:        {ensemble_loss:.3f} / {ensemble_psnr:.3f} / {ensemble_ssim:.3f}')\n",
    "        if multipass:\n",
    "            print(f'+ Multi-Pass [{denoising_passes}]:  {multipass_loss:.3f} / {multipass_psnr:.3f} / {multipass_ssim:.3f}')\n",
    "\n",
    "\n",
    "    # plot the last input-denoised pair\n",
    "    fig, axes = plt.subplots(1, 3 + gt + multipass, figsize=(18,8), sharey=True, sharex=True)\n",
    "\n",
    "    axes[0].imshow(tens2img(stack[[0]]), cmap='gray', vmin=0, vmax=1)\n",
    "    axes[1].imshow(tens2img(no_ensemble_denoised), cmap='gray', vmin=0, vmax=255)\n",
    "    axes[2].imshow(tens2img(single_pass_denoised), cmap='gray', vmin=0, vmax=255)\n",
    "\n",
    "    axes[0].set_title('Input')\n",
    "    axes[1].set_title('Denoised')\n",
    "    axes[2].set_title('+ Self Ensemble')\n",
    "\n",
    "    if multipass:\n",
    "        axes[3].imshow(tens2img(denoised), cmap='gray', vmin=0, vmax=255)\n",
    "        axes[3].set_title(f'+ Multi-Pass [{denoising_passes}]')\n",
    "\n",
    "    if gt:\n",
    "        axes[-1].imshow(tens2img(ground_truth), cmap='gray', vmin=0, vmax=255)\n",
    "        axes[-1].set_title('Ground Truth')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d8cb7591",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the existing model state from e:\\JKU Bachelor\\Computer Vision\\AOS_Project\\code\\tmp\\submission_model.pth\n",
      "Detected folder as input path, loading all files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating output(s):   0%|          | 1/1064 [00:02<50:59,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                   L1 Loss / PSNR / SSIM:\n",
      "Simple denoised:   10.404 / 21.084 / 0.852\n",
      "+ Ensemble:        9.272 / 22.211 / 0.868\n",
      "+ Multi-Pass [2]:  5.420 / 26.529 / 0.925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating output(s):   1%|          | 6/1064 [00:15<44:42,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                   L1 Loss / PSNR / SSIM:\n",
      "Simple denoised:   6.179 / 26.039 / 0.904\n",
      "+ Ensemble:        5.879 / 26.501 / 0.915\n",
      "+ Multi-Pass [2]:  3.085 / 31.746 / 0.960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating output(s):   1%|          | 11/1064 [00:28<45:29,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                   L1 Loss / PSNR / SSIM:\n",
      "Simple denoised:   5.768 / 27.716 / 0.904\n",
      "+ Ensemble:        5.544 / 28.140 / 0.913\n",
      "+ Multi-Pass [2]:  2.786 / 33.106 / 0.960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating output(s):   1%|â–         | 15/1064 [00:39<45:53,  2.63s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[45], line 203\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    200\u001b[0m stack \u001b[38;5;241m=\u001b[39m load_tiff(f, focal_idx) \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtiff\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m load_image(f, focal_idx)\n\u001b[0;32m    201\u001b[0m stack \u001b[38;5;241m=\u001b[39m preprocess(stack)\n\u001b[1;32m--> 203\u001b[0m no_ensemble_denoised, single_pass_denoised, denoised \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(postprocess, \u001b[43mself_ensemble\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstack\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenoising_passes\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    205\u001b[0m \u001b[38;5;66;03m# If a ground truth image is present, calculate key metrics. Useful for getting an overview of the model on some dataset\u001b[39;00m\n\u001b[0;32m    206\u001b[0m gt_path \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mremovesuffix(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mintegral.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatatype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgt.png\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\cvp\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\cvp\\Lib\\site-packages\\torch\\amp\\autocast_mode.py:16\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[1;32m---> 16\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[45], line 131\u001b[0m, in \u001b[0;36mself_ensemble\u001b[1;34m(model, stack, passes)\u001b[0m\n\u001b[0;32m    127\u001b[0m integral \u001b[38;5;241m=\u001b[39m integral[\u001b[38;5;28;01mNone\u001b[39;00m,:]  \u001b[38;5;66;03m# see each integral as an individual grayscale image\u001b[39;00m\n\u001b[0;32m    129\u001b[0m versions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(get_rots(integral))\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m--> 131\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mversions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m no_ensemble_denoised \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# assume the first integral in the stack as the training height\u001b[39;00m\n\u001b[0;32m    134\u001b[0m     no_ensemble_denoised \u001b[38;5;241m=\u001b[39m preds[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mclone()  \u001b[38;5;66;03m# store it's unmodified prediction for comparison\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\cvp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\cvp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32me:\\JKU Bachelor\\Computer Vision\\AOS_Project\\code\\swin2sr.py:1000\u001b[0m, in \u001b[0;36mSwin2SR.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    997\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    998\u001b[0m     \u001b[38;5;66;03m# for image denoising and JPEG compression artifact reduction\u001b[39;00m\n\u001b[0;32m    999\u001b[0m     x_first \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_first(x)\n\u001b[1;32m-> 1000\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_after_body(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_first\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m+\u001b[39m x_first\n\u001b[0;32m   1001\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_last(res)\n\u001b[0;32m   1002\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_gray(x)\n",
      "File \u001b[1;32me:\\JKU Bachelor\\Computer Vision\\AOS_Project\\code\\swin2sr.py:918\u001b[0m, in \u001b[0;36mSwin2SR.forward_features\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    915\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_drop(x)\n\u001b[0;32m    917\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m--> 918\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatches_resolution\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#x_size)\u001b[39;00m\n\u001b[0;32m    920\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)  \u001b[38;5;66;03m# B L C\u001b[39;00m\n\u001b[0;32m    921\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_unembed(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatches_resolution) \u001b[38;5;66;03m#x_size)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\cvp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\cvp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32me:\\JKU Bachelor\\Computer Vision\\AOS_Project\\code\\swin2sr.py:562\u001b[0m, in \u001b[0;36mRSTB.forward\u001b[1;34m(self, x, x_size)\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, x_size):\n\u001b[1;32m--> 562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_embed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_unembed(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresidual_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_size\u001b[49m\u001b[43m)\u001b[49m, x_size))) \u001b[38;5;241m+\u001b[39m x\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\cvp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\cvp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32me:\\JKU Bachelor\\Computer Vision\\AOS_Project\\code\\swin2sr.py:428\u001b[0m, in \u001b[0;36mBasicLayer.forward\u001b[1;34m(self, x, x_size)\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m    427\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_checkpoint:\n\u001b[1;32m--> 428\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mcheckpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_reentrant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# ADDED use_reentrant=False  to remove warning\u001b[39;00m\n\u001b[0;32m    429\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    430\u001b[0m         x \u001b[38;5;241m=\u001b[39m blk(x, x_size)\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\cvp\\Lib\\site-packages\\torch\\_compile.py:24\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dynamo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\cvp\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:328\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    326\u001b[0m dynamic_ctx\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__enter__\u001b[39m()\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\cvp\\Lib\\site-packages\\torch\\_dynamo\\external_utils.py:17\u001b[0m, in \u001b[0;36mwrap_inline.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\cvp\\Lib\\site-packages\\torch\\utils\\checkpoint.py:458\u001b[0m, in \u001b[0;36mcheckpoint\u001b[1;34m(function, use_reentrant, context_fn, determinism_check, debug, *args, **kwargs)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[38;5;66;03m# Runs pre-forward logic\u001b[39;00m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28mnext\u001b[39m(gen)\n\u001b[1;32m--> 458\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;66;03m# Runs post-forward logic\u001b[39;00m\n\u001b[0;32m    460\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\cvp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\cvp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32me:\\JKU Bachelor\\Computer Vision\\AOS_Project\\code\\swin2sr.py:292\u001b[0m, in \u001b[0;36mSwinTransformerBlock.forward\u001b[1;34m(self, x, x_size)\u001b[0m\n\u001b[0;32m    290\u001b[0m     attn_windows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(x_windows, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_mask)  \u001b[38;5;66;03m# nW*B, window_size*window_size, C\u001b[39;00m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 292\u001b[0m     attn_windows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(x_windows, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_size\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdevice))\n\u001b[0;32m    294\u001b[0m \u001b[38;5;66;03m# merge windows\u001b[39;00m\n\u001b[0;32m    295\u001b[0m attn_windows \u001b[38;5;241m=\u001b[39m attn_windows\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size, C)\n",
      "File \u001b[1;32me:\\JKU Bachelor\\Computer Vision\\AOS_Project\\code\\swin2sr.py:266\u001b[0m, in \u001b[0;36mSwinTransformerBlock.calculate_mask\u001b[1;34m(self, x_size)\u001b[0m\n\u001b[0;32m    264\u001b[0m mask_windows \u001b[38;5;241m=\u001b[39m mask_windows\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size)\n\u001b[0;32m    265\u001b[0m attn_mask \u001b[38;5;241m=\u001b[39m mask_windows\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m-\u001b[39m mask_windows\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 266\u001b[0m attn_mask \u001b[38;5;241m=\u001b[39m \u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_fill\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m100.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_fill\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attn_mask\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
