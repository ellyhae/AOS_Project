{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To be used in the AOS repository at AOS\\AOS for Drone Swarms\\LFR\\python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The directory 'C:\\Users\\chris\\Documents\\JKU\\ComputerVision\\new_ints' already exists.\n"
     ]
    }
   ],
   "source": [
    "## Import libraries section ##\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import shutil\n",
    "import math\n",
    "from LFR_utils import read_poses_and_images,pose_to_virtualcamera, init_aos, init_window\n",
    "import LFR_utils as utils\n",
    "import pyaos\n",
    "import glm\n",
    "\n",
    "import re\n",
    "import glob\n",
    "\n",
    "\n",
    "input_path = r\"C:\\Users\\chris\\Documents\\JKU\\ComputerVision\\Part2\"                                       # path to a downloaded sample batch\n",
    "Integral_Path = r'C:\\Users\\chris\\Documents\\JKU\\ComputerVision\\new_ints'                                       # path to the directory where you want to save the results.\n",
    "Focal_planes = np.arange(4) * 0.4       # List of focal planes for focal stack. Focal plane is set to the ground so it is zero. Negative numbers to go up\n",
    "\n",
    "missing_output = os.path.join(Integral_Path, 'missing.txt')        # file for saving ids of corrupt samples, i.e. ones with fewer than 13 files\n",
    "\n",
    "# Check if the directory already exists\n",
    "if not os.path.exists(Integral_Path):\n",
    "    os.mkdir(Integral_Path)\n",
    "else:\n",
    "    print(f\"The directory '{Integral_Path}' already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################Start the AOS Renderer###############################################################\n",
    "w,h,fovDegrees = 512, 512, 50 # # resolution and field of view. This should not be changed.\n",
    "render_fov = 50\n",
    "\n",
    "if 'window' not in locals() or window == None:\n",
    "                                    \n",
    "    window = pyaos.PyGlfwWindow( w, h, 'AOS' )  \n",
    "     \n",
    "aos = pyaos.PyAOS(w,h,fovDegrees) \n",
    "\n",
    "\n",
    "set_folder = r'.'          # Enter path to your LFR/python directory\n",
    "aos.loadDEM( os.path.join(set_folder,'zero_plane.obj'))\n",
    "\n",
    "####################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################Create Poses for Initial Positions###############################################################\n",
    "\n",
    "# Below are certain functions required to convert the poses to a certain format to be compatabile with the AOS Renderer.\n",
    "\n",
    "def eul2rotm(theta) :\n",
    "    s_1 = math.sin(theta[0])\n",
    "    c_1 = math.cos(theta[0]) \n",
    "    s_2 = math.sin(theta[1]) \n",
    "    c_2 = math.cos(theta[1]) \n",
    "    s_3 = math.sin(theta[2]) \n",
    "    c_3 = math.cos(theta[2])\n",
    "    rotm = np.identity(3)\n",
    "    rotm[0,0] =  c_1*c_2\n",
    "    rotm[0,1] =  c_1*s_2*s_3 - s_1*c_3\n",
    "    rotm[0,2] =  c_1*s_2*c_3 + s_1*s_3\n",
    "\n",
    "    rotm[1,0] =  s_1*c_2\n",
    "    rotm[1,1] =  s_1*s_2*s_3 + c_1*c_3\n",
    "    rotm[1,2] =  s_1*s_2*c_3 - c_1*s_3\n",
    "\n",
    "    rotm[2,0] = -s_2\n",
    "    rotm[2,1] =  c_2*s_3\n",
    "    rotm[2,2] =  c_2*c_3        \n",
    "\n",
    "    return rotm\n",
    "\n",
    "def createviewmateuler(eulerang, camLocation):\n",
    "    \n",
    "    rotationmat = eul2rotm(eulerang)\n",
    "    translVec =  np.reshape((-camLocation @ rotationmat),(3,1))\n",
    "    conjoinedmat = (np.append(np.transpose(rotationmat), translVec, axis=1))\n",
    "    return conjoinedmat\n",
    "\n",
    "def divide_by_alpha(rimg2):\n",
    "        a = np.stack((rimg2[:,:,3],rimg2[:,:,3],rimg2[:,:,3]),axis=-1)\n",
    "        return rimg2[:,:,:3]/a\n",
    "\n",
    "def pose_to_virtualcamera(vpose ):\n",
    "    vp = glm.mat4(*np.array(vpose).transpose().flatten())\n",
    "    #vp = vpose.copy()\n",
    "    ivp = glm.inverse(glm.transpose(vp))\n",
    "    #ivp = glm.inverse(vpose)\n",
    "    Posvec = glm.vec3(ivp[3])\n",
    "    Upvec = glm.vec3(ivp[1])\n",
    "    FrontVec = glm.vec3(ivp[2])\n",
    "    lookAt = glm.lookAt(Posvec, Posvec + FrontVec, Upvec)\n",
    "    cameraviewarr = np.asarray(lookAt)\n",
    "    #print(cameraviewarr)\n",
    "    return cameraviewarr  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## Below we generate the poses for rendering #####################################\n",
    "# This is based on how renderer is implemented. \n",
    "\n",
    "Numberofimages = 11  # Or just the number of images\n",
    "\n",
    "# ref_loc is the reference location or the poses of the images. The poses are the same for the dataset and therefore only the images have to be replaced.\n",
    "\n",
    "ref_loc = [[5,4,3,2,1,0,-1,-2,-3,-4,-5],[0,0,0,0,0,0,0,0,0,0,0]]   # These are the x and y positions of the images. It is of the form [[x_positions],[y_positions]]\n",
    "\n",
    "altitude_list = [35,35,35,35,35,35,35,35,35,35,35] # [Z values which is the height]\n",
    "\n",
    "center_index = 5  # this is important, this will be the pose index at which the integration should happen. For example if you have 5 images, lets say you want to integrate all 5 images to the second image position. Then your center_index is 1 as index starts from zero.\n",
    "\n",
    "site_poses = []\n",
    "for i in range(Numberofimages):\n",
    "    EastCentered = (ref_loc[0][i] - 0.0) #Get MeanEast and Set MeanEast\n",
    "    NorthCentered = (0.0 - ref_loc[1][i]) #Get MeanNorth and Set MeanNorth\n",
    "    M = createviewmateuler(np.array([0.0, 0.0, 0.0]),np.array( [ref_loc[0][i], ref_loc[1][i], - altitude_list[i]] ))\n",
    "    #print('m',M)\n",
    "    ViewMatrix = np.vstack((M, np.array([0.0,0.0,0.0,1.0],dtype=np.float32)))\n",
    "    #print(ViewMatrix)\n",
    "    camerapose = np.asarray(ViewMatrix.transpose(),dtype=np.float32)\n",
    "    #print(camerapose)\n",
    "    site_poses.append(camerapose)  # site_poses is a list now containing all the poses of all the images in a certain format that is accecpted by the renderer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = re.compile(r'(\\d+)')\n",
    "def getNumbers(value):\n",
    "    parts = numbers.split(value)\n",
    "    parts = list(map(int, parts[1::2]))\n",
    "    return parts\n",
    "\n",
    "dir_files = sorted(os.listdir(input_path), key=getNumbers)\n",
    "\n",
    "index = 0\n",
    "while index < len(dir_files):\n",
    "    cur_bi = getNumbers(dir_files[index])[:2]\n",
    "    files = list(filter(lambda f: getNumbers(f)[:2] == cur_bi, dir_files[index:index+13]))\n",
    "    \n",
    "    if len(files) != 13:\n",
    "        index += len(files)\n",
    "        with open(missing_output, 'a') as missing_f:\n",
    "            missing_f.write(f'{cur_bi[0]} {cur_bi[1]}\\n')\n",
    "        continue\n",
    "        \n",
    "    files = [os.path.join(input_path, f) for f in files]\n",
    "        \n",
    "    parameters, ground_truth, images = files[0], files[1], files[2:]\n",
    "    \n",
    "    imagelist = list(map(cv2.imread, images))\n",
    "    \n",
    "    with open(parameters) as f:\n",
    "        params = f.read().splitlines()\n",
    "    pose = params[17]\n",
    "    if pose != '':\n",
    "        x, y = pose.split(' ')[-6:-4]\n",
    "    else:\n",
    "        x = y = 'N'\n",
    "    \n",
    "    batch_index_x_y = '_'.join([str(cur_bi[0]), str(cur_bi[1]), x, y])\n",
    "    \n",
    "    aos.clearViews()   # Every time you call the renderer you should use this line to clear the previous views  \n",
    "    for i in range(len(imagelist)):\n",
    "        aos.addView(imagelist[i], site_poses[i], \"DEM BlobTrack\")  # Here we are adding images to the renderer one by one.\n",
    "    \n",
    "    integral_stack = np.zeros((len(Focal_planes), w, h), np.float32)\n",
    "    \n",
    "    for i, Focal_plane in enumerate(Focal_planes):\n",
    "        aos.setDEMTransform([0,0,Focal_plane])\n",
    "    \n",
    "        proj_RGBimg = aos.render(pose_to_virtualcamera(site_poses[center_index]), render_fov)\n",
    "        integral_stack[i] = divide_by_alpha(proj_RGBimg)[...,0]   # only save one channel, as grayscale has the same value for r, g and b, effectively wasting memory\n",
    "    \n",
    "    ok = cv2.imwritemulti(os.path.join( Integral_Path, batch_index_x_y + '_integral.tiff'), integral_stack.round().astype(np.uint8))\n",
    "    \n",
    "    if not ok:\n",
    "        raise IOError(f\"Error writing Output for inputs: batch={cur_bi[0]}, id={cur_bi[1]}, loop index={index}\")\n",
    "        \n",
    "    assert os.path.basename(ground_truth).split('_')[2] == 'GT', index\n",
    "    shutil.copyfile(ground_truth, os.path.join(Integral_Path, batch_index_x_y + '_gt.png'))\n",
    "    \n",
    "    index += 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvproj",
   "language": "python",
   "name": "conda-env-cvproj-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "c0546a8624a4a236bae0f9fea37c96b2936c9ad1821cd89b71f7783537db0568"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
